{"posts":[{"title":"【Auto-Task项目总结】","text":"笔记作者：陆思宇 方正坤 王昕凯@无糖实习笔记小编：黄诚@安全学术圈 1、研究介绍 Auto-Task 是一个实验性的开源应用程序，展示了 GPT-4 语言模型的能力。该程序由LLM大语言模型驱动，通过连接语言模型的”思考”，自主地实现你设定的任何目标。它展示了 大预言模型在自主决策和执行任务方面的潜力。 Auto-Task利用大语言模型强大的自然语言处理能力，能够理解和生成自然语言文本。它可以接收用户设定的目标，并通过分析和推理生成一系列步骤，以实现这些目标。无论是撰写文章、解答问题、制定计划，还是执行复杂的任务，Auto-Task 都能通过与用户的对话来理解需求，并提供相应的解决方案。 Auto-Task的灵活性和适应性使其成为各种场景中的有用工具。它可以用于个人的日常事务管理，例如制定旅行计划、组织日程安排或寻求创意灵感。在安全领域，Auto-Task主要的功能可以协助漏洞发布信息的相关搜集以及相关论文的自动化理解和分析，可以针对使用者的问题给出相对客观的答案。 Auto-Task 的开源性质使得使用者能够自由探索和扩展其功能。它提供了一种研究和实验的平台，以进一步探索自然语言处理和人工智能领域的可能性。 总之，Auto-Task 是一个令人印象深刻的应用程序，展示了大语言模型的创新能力。它通过自主连接大语言模型的逻辑推理功能，实现用户设定的目标。为之后安全方向的实践做了一定程度的探索。 2、构建思路 prompt模块 prompt模块是auto-task类项目存在的核心和关键要素，该类型项目的主要功能便是基于大语言模型的理解能力按特定的格式命令宿主操作系统去执行一定的工作和任务，所以prompt的模式和构造条件十分重要。 构造： 常规prompt： 1234567891011121314151617181920212223242526272829303132333435You are Bob, AI Your decisions must always be made independently without seeking user assistance. The OS you are running on is: windowsGOALS:1.总结之前的所有内容回答问题并输出在返回格式的&quot;text&quot;中，不执行命令Constraints: 1.Exclusively use the commands listed in double quotes e.g. &quot;command name&quot;Commands: 1.get html info: get html info, args: &quot;url&quot;: &quot;&lt;url&gt;&quot; 2.download paper: DOWNLOAD PAPER BY KEYWORD, args: &quot;keyword&quot;: &quot;&lt;keyword&gt;&quot; 3.google: Google Search, args: &quot;query&quot;: &quot;&lt;query&gt;&quot;Resources:current finished: You should only respond in JSON format as described below Response Format: { &quot;thoughts&quot;: { &quot;text&quot;: &quot;what you thought,this part can be long&quot;, &quot;plan&quot;: &quot;- short bulleted\\n- list that conveys\\n- long-term plan&quot; }, &quot;command&quot;: { &quot;name&quot;: &quot;command name&quot;, &quot;args&quot;: { &quot;arg name&quot;: &quot;value&quot; } } } Ensure the response can be parsed by Python json.loads 我们可以把一个prompt分割为四个部分，说明、限制、命令、返回格式： 1.说明：阐述你需要llm扮演的角色或使用场景 2.限制：限制命令选择及任务限制必须在已有的命令条件下完成 3.命令：可以允许选择的用来解决问题的命令 4.返回格式：包括陈述，计划和命令控制，为json格式。 prompt运行案例： 例如上述的prompt中当你向llm询问怎么得知BBC今日报道时，其返回结果如下所示： 123456789101112{ &quot;thoughts&quot;: { &quot;text&quot;: &quot;Based on the BBC homepage HTML, here are the top news headlines today:&quot;, &quot;plan&quot;: &quot;- Report the top headlines and summaries from [bbc.com](http://bbc.com/) homepage&quot; }, &quot;command&quot;: { &quot;name&quot;: &quot;get html info&quot;, &quot;args&quot;: { &quot;url&quot;: &quot;[https://www.bbc.com/](https://www.bbc.com/)&quot; } } } 可以观察到llm告诉了我们它要获取BBC的报道的plan以及陈述，并返回了命令控制符，name为get html info，网站为Home - BBC News，于是操作系统便访问BBC主页分析并总结内容，llm再决定下一步的计划。 1234567891011121314151617181920212223242526class PromptGenerator: def __init__(self) -&gt; None: self.constraints = [ Exclusively use the commands listed in double quotes e.g. &quot;command name&quot; ] self.commands = [] self.resources = [] self.performance_evaluation = [] self.goals = [] self.command_registry = None self.name = &quot;Bob&quot; self.role = &quot;AI&quot; self.response_format = { &quot;thoughts&quot;: { &quot;text&quot;: &quot;what you thought,this part can be long&quot;, &quot;plan&quot;: &quot;- short bulleted\\n- list that conveys\\n- long-term plan&quot;, }, &quot;command&quot;: { &quot;name&quot;: &quot;command name&quot;, &quot;args&quot;: {&quot;arg name&quot;: &quot;value&quot; } }, } 在每一步prompt构造时只需修改该对象中的内容，最后通过读取该对象的内容来生成最新需要的prompt即可。 命令目录及插件管理 12345678910111213141516171819202122232425262728def command(name: str, description: str, signature: str = &quot;&quot;, enabled: bool = True, disabled_reason: Optional[str] = None, ) def decorator(func: Callable[..., Any]) -&gt; Command:cmd = Command( name=name, description=description, method=func, signature=signature, enabled=enabled, disabled_reason=disabled_reason, ) @functools.wraps(func) def wrapper(*args, **kwargs) -&gt; Any: return func(*args, **kwargs) wrapper.command = cmd setattr(wrapper, AUTO_GPT_COMMAND_IDENTIFIER, True) return wrapper return decorator 当有新插件更新到项目时，仅需在插件参考中加入导入插件的python文件，即可自动导入文件中使用了如上装饰函数装饰的函数进可调用命令的目录中。 命令体构建 prompt运行逻辑 本项目目前实现的功能主要为联网查询及论文内容总结，为解决auto-gpt任务分解过盛的问题，我们采用简单的剩余任务分解数绑定任务类型的方式进行分解，举例：初始任务分解数为2，允许自主执行两次任务，llm首先执行搜索引擎的搜索工作，任务分解数+1=3，llm分析之后选取某一个网页进行爬取并得到内容总结，任务分解数-1=2，随后llm想根据已有知识总结得到答案，任务分解数-1-1=0，退出命令执行输出结果； memory模块 存储功能 存储本任务已执行操作的结果，包括论文总结，网页总结，命令执行记录 总结调用 当需要调用总结命令时，会把这些记录作为参考源提供给llm 文本处理（text_processing）模块总结summarize_text()函数是text_processing模块中的一个辅助函数，功能为被其他模块的函数调用，接受传入的一段长度任意的文本字符串和指定的问题，并返回由模型基于这一段文本针对这一指定问题进行的总结。 12345678910111213141516171819202122def summarize_text(text: str, question: str) -&gt; str: if not text: return &quot;Error: No text to summarize&quot; summaries = [] chunks = list( split_text(text, max_length=3072, question=question), ) for i, chunk in enumerate(chunks): messages = create_message(chunk, question) try: summary = chat_with_claude(message=messages) summaries.append(summary) time.sleep(10) except Exception as err: return f&quot;Error: {err}&quot; combined_summary = &quot;\\n&quot;.join(summaries) messages = create_message(combined_summary, question) return chat_with_claude(message=messages) 该函数利用split_text()函数将传入的文本分割成长度合适的文本块，然后将其包装成json格式发送给模型进行总结。为了保证最后的总结能够概括传入文本的所有内容，所有分段的总结内容会在最后重新组合在一起发送给模型进行一次总结。 12345678910111213141516171819202122232425262728293031323334import spacyfrom chat import chat_with_claudedef split_text(text: str, max_length: int = 3000, question: str = &quot;&quot;,) -&gt; Generator[str, None, None]: flattened_paragraphs = &quot; &quot;.join(text.split(&quot;\\n&quot;)) nlp = spacy.load(&quot;en_core_web_sm&quot;) nlp.add_pipe(&quot;sentencizer&quot;) doc = nlp(flattened_paragraphs) sentences = [sent.text.strip() for sent in doc.sents] current_chunk = [] for sentence in sentences: message_with_additional_sentence = [ create_message(&quot; &quot;.join(current_chunk) + &quot; &quot; + sentence, question) ] expected_length = count_message_length(message_with_additional_sentence) if expected_length &lt;= max_length: current_chunk.append(sentence) else: yield &quot; &quot;.join(current_chunk) current_chunk = [sentence] message_this_sentence_only = [ create_message(&quot; &quot;.join(current_chunk), question) ] expected_length = len(message_this_sentence_only) + 1 if expected_length &gt; max_length: raise ValueError( f&quot;Sentence is too long in webpage: {expected_length} tokens.&quot; ) if current_chunk: yield &quot; &quot;.join(current_chunk) split_text()函数是一个生成器，在调用spacy进行文本分词后依次生成限制长度的文本块返回给summarize_text()函数在auto-GPT项目中，文本长度衡量和分割的方式是调用tiktoken库，将文本块转化为一段token，并以句子为单位计算当前长度的文本token消耗是否溢出。但tiktoken库是由openAI开发的子词标记化工具，在以chat-GPT为交互模型的项目上有很好的表现，但是在其他大语言模型上，由于其token构建方式的可能存在不同，不能保证使用效果；基于词向量化的文本分割功能开发尚未完全，因此当前项目的分段方式为测算字符串长度。 123456789101112def create_message(chunk: str, question: str) -&gt; Dict[str, str]: return { &quot;content&quot;: f'&quot;&quot;&quot;{chunk}&quot;&quot;&quot; Using the above text, answer the following' f' question: &quot;{question}&quot; -- if the question cannot be answered using the text,' &quot; summarize the text.&quot;, }def count_message_length(messages: list) -&gt; int: length = 0 for message in messages: length += len(str(message)) return length 需要进行总结的文本块会在create_message()函数中被包装成一个json格式的信息，该信息要求模型仅针对给定的文本内容回答问题或总结内容。 插件功能梳理插件实现的通用原理· main函数：123456789command_registry = CommandRegistry()command_categories = [ &quot;crawler_selenium&quot;, &quot;paper_selenium&quot;, &quot;autogpt.commands.google_search&quot;]for command_category in command_categories: command_registry.import_commands(command_category) 新建CommandRegisry类并在command_catagories列表下写入待使用的插件名（文件名），由import_commands()函数导入对应插件，以确保相关函数可以在后续过程中被调用。 · Command模块：在CommandRegistry类下： 1234567891011121314151617def import_commands(self, module_name: str) -&gt; None: module = importlib.import_module(module_name) for attr_name in dir(module): attr = getattr(module, attr_name) # Register decorated functions if hasattr(attr, AUTO_GPT_COMMAND_IDENTIFIER) and getattr( attr, AUTO_GPT_COMMAND_IDENTIFIER ): self.register(attr.command) # Register command classes elif ( inspect.isclass(attr) and issubclass(attr, Command) and attr != Command ): cmd_instance = attr() self.register(cmd_instance) 该函数从导入的模块中的筛选被command装饰器标记的命令插件，并将这些插件注册为Command对象，最终添加到CommandRegistry对象的commands字典中。这样，在后续的操作中，可以通过命令名称从commands字典中获取对应的命令对象，以执行相应的操作。 · 提示工程：在PromptGenerator下： 1234567def __init__(self) -&gt; None: self.constraints = [ '只使用双引号中列出的命令。“命令名”' ] self.commands = [] self.command_registry = None 在生成prompt时，要求模型只能使用注册在命令目录当中的命令，从而保证模型返回的任务执行步骤总能被某个插件响应。命令目录一般在主函数中会进行初始化。 1234567891011121314151617181920def add_command( self, command_label: str, command_name: str, args=None, function: Optional[Callable] = None,) -&gt; None: if args is None: args = {} command_args = {arg_key: arg_value for arg_key, arg_value in args.items()} command = { &quot;label&quot;: command_label, &quot;name&quot;: command_name, &quot;args&quot;: command_args, &quot;function&quot;: function, } self.commands.append(command) 该函数将每个命令创建一个包含标签、名称、参数和可调用函数的字典，并将该命令字典添加到命令列表中。 基于以上操作，所有给定的插件均在项目中完成了安装，并在之后的使用过程中根据模型给出的执行需求选择对应的命令执行。 当前实现的插件种类 谷歌搜索： 使用DuckDuckGo引擎搜索关键词，返回一个保存有搜索结果和访问链接的列表 网页爬取和总结： 爬取指定网页的所有前端文字内容，并根据给定的问题让模型进行总结 论文管理： 在paperswithcode上下载指定关键词的论文，并总结内容 各插件功能的实现方式谷歌搜索DuckDuckGo是一种注重隐私保护的搜索引擎，为用户提供匿名化的搜索体验。该引擎在Python下可直接调用函数提交查询请求而无需使用API-Key，因此相较于直接使用谷歌搜索的API具有更好的安全性和项目搭建效率。 将关键词和结果条数提交给duckduckgo_search()后，函数以列表形式返回搜索结果，然后调用safe_google_search()函数将结果转换为如上所示的json标准格式。 网页爬取 该功能基于Selenium构建爬虫进行网站爬取。访问指定网站加载完成后执行js脚本获取其所有文本内容，然后将这些内容分割到段，再把分割后的文本发送给模型进行内容总结。 论文管理PDFMiner是一个可以从PDF文档中提取信息的工具，可以把PDF文件转换成HTML等格式。通过这个库，可以较为轻松地从PDF文件当中获取文本内容，从而进行后续操作。 论文管理插件主要包含论文批量下载和批量阅读两个功能。下载：当指定关键词后，该插件被调用，在paperswithcode上搜索关键词，然后依次访问并下载前数篇论文到本地指定路径。阅读：从指定路径读取论文，调用PDFMiner对其进行解析，提取文本内容到content字符串中并保存为txt文件（可选），同时将内容发送给模型使其提取摘要。 开源的ChatGPT应用项目查阅和对比本方面我们查阅了大量的开源项目，对其源码进行分析阅读，总结了三个较为突出的开源项目并进行对比： ① ChuanhuChatGPT ​ 川虎为ChatGPT等多种LLM提供了一个轻快好用的Web图形界面和众多附加功能，其旨在为多个大语言模型提供连接接口和自由切换，使得用户可以方便的使用自己部署的大语言模型或者是GPT等模型的 api。在已经开源的项目中，川虎已经部署了主流的GPT的多个版本的模型接口以及其他部分大语言模型接口比如Moss，只要提供相应的api就可以和相应的的模型对话。 ​ 同时，川虎GPT为大语言模型提供了多种补充功能和潜力挖掘，比如可以上传文件，用自带的prompt模板帮助用户获取更准确的答案，可以保存重要的对话并导出保存等等。此外，该系统还提供了用户的个性化服务，便捷用户的使用。 以下是一些该项目的使用技巧： 12345678使用System Prompt可以很有效地设定前提条件。使用Prompt模板功能时，选择Prompt模板集合文件，然后从下拉菜单中选择想要的prompt。如果回答不满意，可以使用 重新生成按钮再试一次输入框支持换行，按 shift enter即可。可以在输入框按上下箭头在输入历史之间切换部署到服务器：在 config.json 中设置 &quot;server_name&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: &lt;你的端口号&gt;,。获取公共链接：在 config.json 中设置 &quot;share&quot;: true,。注意程序必须在运行，才能通过公共链接访问。在Hugging Face上使用：建议在右上角 复制Space 再使用，这样App反应可能会快一点。 ​ 川虎系统的代码逻辑清晰，功能齐全，文件功能分工明确，易读易懂，二次开发的价值和潜力很大。但是正因为代码衔接紧密，因此想要单独剥离功能块的难度较高。川虎GPT本身是一个大语言模型功能的集成，在面向大语言模型应用领域还有很大的挖掘潜力。 ② Hoppin ​ Hoppin为gitee网站上一名作者开源的基于 Java 语言的连接 ChatGPT 的 api 的项目。作者做了一个类GPT的对话聊天系统，同时为用户配置了较为详尽的 api 策略和费用统计。本项目的特点在于作者制作了较为美观的前端，并附加了图片上传和图片生成功能，并为用户提供了较为齐全的身份验证机制和 ChatGPT 的 api 管理策略。以下是一些项目图片。 ​ 上图是项目的主界面，可以看到基本的聊天功能该项目都具备。 ​ 上图是为用户设置的 api 管理功能。 ​ 本项目的前端界面美观，并且功能相对较全，有利于二次开发和进一步的功能拓展。但比较遗憾的是，项目源码的逻辑设计和代码架构并不清晰，有些许混乱，初次接触源码的开发者可能需要一定的时间来梳理代码逻辑，这可能会是一个小小的缺憾。 ③ chatai-vue ​ chatai-vue项目是使用 vue 高仿了 chatgpt 的前端，后端使用 python flask openai 实现。 后续作者又进行了 openai 的 api 更新，开放了最新的 gpt-3.5-turbo 模型，后端使用了最新模型在分支 toGpt3.5 上，加上了流式响应。 新模型更加强大，更加智能。本项目是完全类仿 ChatGPT 的界面制作的，意图使得拥有 api 的用户可以随时随地较为方便的使用ChatGPT的服务。以下是一些项目图展示。 ​ 阅读本项目的源码后，觉得代码的逻辑结构清晰，功能设计条理，有很大的二次开发价值，因而，我们后续的项目便基于 chatai-vue 项目进行进一步的研发。 Auto-task前后端服务搭建​ 本任务是前端框架和后端服务器的搭建，前端框架中使用 vue 高仿了一个类似 ChatGPT 的前端，服务器端的代码主要使用 Node.js和Express框架实现。 ​ Vue.js（或简称为Vue）是一个用于创建用户界面的开源JavaScript框架，也是一个创建单页应用的Web应用框架。Vue所关注的核心是MVC模式中的视图层，同时，它也能方便地获取数据更新，并通过组件内部特定的方法实现视图与模型的交互。 1MVC全名是Model View Controller，是模型(model)－视图(view)－控制器(controller)的缩写，一种软件设计典范，用一种业务逻辑、数据、界面显示分离的方法组织代码，将业务逻辑聚集到一个部件里面，在改进和个性化定制界面及用户交互的同时，不需要重新编写业务逻辑。MVC被独特的发展起来用于映射传统的输入、处理和输出功能在一个逻辑的图形化用户界面的结构中。 项目部署： vue 前端： 下载依赖： 1npm i 项目运行： 1npm run dev 服务器sever.js文件，直接运行： 项目结构： ​ 就具体的业务逻辑，我们对每个新会话进行标注 cid ，并对会话中的每一次问答进行编号标注 idx ，并设定一些规则，例如[DONE]为服务器端回复内容的结束标志等等来方便后续的开发。 项目展示： 3、团队思考 本项目团队由三位成员组成，我们致力于团队合作，并积极尝试了大语言模型在安全领域的探索应用。auto-task是以大语言模型为驱动的自动化任务处理工程，对于使用者的需求生成相应的propmt方案交由大语言模型理解并根据模型返回信息执行命令并决定之后的任务方案，团队在网页关键信息搜集，论文下载及总结，搜索引擎信息检索等方面作出了一定的努力，团队认为大语言模型的代码分析能力以及逻辑推理能力在安全领域具有十分广泛的应用价值，值得进一步的深入和研究。","link":"/2023/06/24/%E3%80%90Auto-Task%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%E3%80%91/"},{"title":"【总结】LLM论文阅读（Finetune篇）-第二周","text":"Fine Tuning 默认情况下对模型所有参数进行调整 也泛指对模型参数进行微调以适应下游任务的技术 Parameter-Efficient Fine-Tuning（PEFT） P-Tuning（v2） Prefix Tuning Prompt Tuning LoRA Prompt硬提示/离散提示（Hard Prompt/Discrete Prompt）硬提示就是指人为设计上面提到的Prompt。硬提示一般需要模型在这个域上有比较多的经验，并且使用前需要知道这个模型的底层是什么样的。否则，硬提示的性能一般会比Fine-tuning的SOTA差很多。 人工构建（Manual Template） 启发式法（Heuristic-based Template） 通过规则、启发式搜索等方法构建合适的模板； 生成（Generation） 根据给定的任务训练数据（通常是小样本场景），生成出合适的模板 软提示/连续提示（Soft Prompt/Continuous Prompt）（Prompt Tuning）软提示把Prompt的生成本身作为一个任务进行学习，相当于把Prompt的生成从人类一个一个尝试（离散）变换成机器自己进行学习、尝试（连续）。 Prefix-tuning由Li等人在2021年提出，在生成式任务中效果优秀。这一方法保留了模型参数，并通过反向传播错误，调整每个encoder层和输入层前插入的prefix激活函数。从而对不同任务的微调模型只需要保存其对应的prefix参数。 每个任务唯一对应一套prefix，不支持多任务。 Prompt-Tuning保留完整预训练模型，仅允许每个下游任务有额外的k个可调整token置入于输入文本的头部。该方法和prefix tuning的差别在于——他没有在中间层插入prefix或添加额外的输出层，仅仅对输入的prompt进行了调整。 实现 “Span Corruption”：基于T5的预训练模型进行实现，T5的预训练进行句子空缺内容的预测，使用Span标注空缺位置，下面举个例子。 输入：Thank you 〈X〉 me to your party 〈Y〉 week 输出：〈X〉 for inviting 〈Y〉 last 〈Z〉 其中输入中类似〈X〉的哨兵标注了空缺，输出表示对输入内容的填补，同样使用哨兵作为输出结尾 ”Span Corruption + Sentinel“：prompt tuning为了接近输入在预训练中的状态，微调时，向所有下游任务头部插入一个哨兵 “LM Adaptation”：延续T5进行少量额外的自监督训练，但以“LM”为目标（即Transformer中的解码器，基于上文预测下一个出现的token）。 作者希望通过LM Adaptation，把模型速转换成一个更类似GPT-3的模型。 由于这种方法与从头开始预训练的效果从未被比较过，文中尝试了不同Step下的Adaptation效果。 LoRa（Low-Rank Adaptation）现有方法缺点 classic fine-tune：完全更新模型参数 缺点：参数量过大，效率低下，存储所有微调后的模型相当困难 Adapter：在模型网络中插入Adapter层，在不影响原参数的情况下，对Adapter层内参数进行更新 缺点：设计引入了串行步骤，造成推理延迟“显著”提高（3%~30%，序列越短越明显），需要在效率和模型质量之间权衡 prefix tuning 缺点：优化难度大，其性能在可训练参数增加时非单调变化。更重要的是，固定一部分序列长度必然会导致下游任务可用的序列长度减少，可能导致模型性能不如其他方法。 方法 特点 不改变预训练模型权重，而在模型外添加旁路，用训练旁路中的矩阵模拟微调过程。 优点 不引入额外延迟，只需要将旁路参数与模型参数合并，用新参数推理结果即可。 假设 模型权重的更新矩阵有一个低秩 本征维度intrinsic dimensionality定义：目标函数达到精确解决优化问题所需的最小维度 在2020年，一项由Aghajanyan等人进行的研究衡量了模型微调的本征维度，尝试寻找对每个任务进行微调时需要多少空余参数才能大概解决优化问题 他们提出——预训练本身是在为下游NLP任务最小化本征维度，预训练实际上是一个学习压缩平均NLP任务的框架 本征维度计算计算目标函数的精确本征维度非常困难，但可以使用启发式方法计算上界。 假设 $\\theta^D=[\\theta_0,\\theta_1,…,\\theta_m]$ 是模型 $f(·,\\theta)$ 的D个参数。相比直接优化模型参数 $\\theta^D$ 的损失函数，我们只关注参数的子空间，通过参数化d（d &lt; D）个维度对模型进行微调： $$\\theta^D=\\theta^D_0+P(\\theta^d)$$ 其中 $P:\\mathbb{R}^d\\rightarrow\\mathbb{R}^D$，将参数从低维d映射到高维D。微调过程中，只有 $\\theta^d$ 是变量，其他都是定值。 具体实现定义达到训练完整权重矩阵效果的90%为令人满意的效果（如果完整模型训练后准确率85%，那么调整后目标准确率为0.9*85%=76.5%），称这个最小维度为 $d_{90}$。 DID. 随机从所有参数中挑选 $\\theta^d$ ，称这种方法找的的本征维度为Direct Intrinsic Dimension $$\\theta^D=\\theta^D_0+\\theta^dM，M=HG\\Pi HB$$ SAID. 对模型的每一层参数单独进行映射，以参数之和作为本征维度Structure Aware Intrinsic Dimension $$\\theta^D_i=\\theta^D_{0,i}+\\lambda_i P(\\theta^{d-m})_i$$ 本征维度计算结果 MRPC：包含约3700个训练样本的段落文本语义一致性预测任务 QQP：包含约363k个训练样本的问题文本语义一致性预测任务 结论 预训练模型效果越好，本征维度越小 训练集规模越大，本征维度越大 重新理解LoRA 现有预训练权重矩阵 $W_0\\in\\mathbb{R}^{d\\times k}$ ，将权重更新表示为 $W_0+\\bigtriangleup W=W_0+BA$ 其中 $B\\in\\mathbb{R}^{d\\times r},A\\in\\mathbb{R}^{r\\times k}$，秩 $r\\ll min(d,k)$（可任意取值）。 在训练中，预训练权重矩阵保持不变，不接受梯度更新，而将$A,B$作为 训练参数矩阵。 使用高斯随机函数初始化A，使用0初始化B，因此$\\bigtriangleup W$在训练开始时为0。 实现细节在Transformer结构中，自注意力模块有4个权重矩阵 $W_q,W_k,W_v,W_o$，MLP（多层感知器，一种前向神经网络）模块有2个权重矩阵。实验中，LoRA只微调 $W_q,W_k,W_v$ 中的一个，相当于为下游任务调整时只变更注意力权重，而不变化MLP模块。 所有实验中，取秩$r=4$ （在1~64的取值中效果较好）。 实验效果 Chinese-LLaMA-Alpaca项目地址：https://github.com/ymcui/Chinese-LLaMA-Alpaca LLaMA：2023年Facebook Meta发布的最新Transformer decoder大语言模型，主要基于英文数据进行训练，采用了包括预归一化、SwiGLU 激活函数、旋转嵌入等一系列优化措施。 Chinese-LLaMA-Alpaca 增加2000中文词汇，增强LLaMA的中文理解能力 采用LoRA进行模型高效训练和部署 Pre-traing Stage-1Chinese-LLaMA没有完全训练一个新模型，而是以原LLaMA得到权重作为初始状态，在中文数据集上继续进行预训练。 中文数据集（20G），仅有LLaMA训练语料大小的0.5% 方法 固定Transformer encoder中的参数——最小对化模型的扰动 训练embedding层参数——适应新添加的中文词向量 Pre-traing Stage-2方法 使用LoRA，并行调整Transformer注意力模块权重 训练embedding层和LM head层 LM head层：出现在大部分类GPT结构中，作为模型输出层，将hidden_states张量的最后一个维度映射到词典维度。 Instruction Fine-tuning由于微调采用的是 Stanford Alpaca 提出的对LLaMA的自指导微调方法来训练指令服从模型，此阶段产生的模型称为Chinese Alpaca。 方法 使用ChatGPT基于少量的指令数据Prompt，迭代进行指令数据的生成 使用LoRA，并行调整Transformer MLP模块权重 参考文献 Liu X, Zheng Y, Du Z, et al. GPT understands, too[J]. arXiv preprint arXiv:2103.10385, 2021. Liu X, Ji K, Fu Y, et al. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks[J]. arXiv preprint arXiv:2110.07602, 2021. Li X L, Liang P. Prefix-tuning: Optimizing continuous prompts for generation[J]. arXiv preprint arXiv:2101.00190, 2021. Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient prompt tuning[J]. arXiv preprint arXiv:2104.08691, 2021. Aghajanyan A, Zettlemoyer L, Gupta S. Intrinsic dimensionality explains the effectiveness of language model fine-tuning[J]. arXiv preprint arXiv:2012.13255, 2020. Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021. Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models[J]. arXiv preprint arXiv:2302.13971, 2023. Wang Y, Kordi Y, Mishra S, et al. Self-Instruct: Aligning Language Model with Self Generated Instructions[J]. arXiv preprint arXiv:2212.10560, 2022. Cui Y, Yang Z, Yao X. Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca[J]. arXiv preprint arXiv:2304.08177, 2023.","link":"/2023/06/13/%E3%80%90%E6%80%BB%E7%BB%93%E3%80%91LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88Finetune%E7%AF%87%EF%BC%89-%E7%AC%AC%E4%BA%8C%E5%91%A8/"},{"title":"【总结】LLM论文阅读（入门篇）-第一周","text":"深度学习和TransformerRNN和LSTM都是通过递归的方式处理序列，即当前的隐藏状态是基于前一时间步的隐藏状态和当前的输入来计算。 循环神经网络 RNN通过将隐藏状态从一个时间步传递到下一个时间步，以此捕获序列中的信息 长短期记忆 LSTM引入了一种叫做“门”的机制来控制信息的流动，这使得LSTM解决RNN的梯度消失问题，能够捕获更长距离依赖。 TransformerSelf-Attention Mechanism 为每个元素分配一个权重，这个权重反映了其他元素对当前元素的重要性。 使用权重矩阵对每个元素生成query和key 对单个元素，算出该元素的query和其他元素的key的点积，运用softmax函数算出注意力分数 对注意力分数进行加权求和 Multi-Head Self-Attention 通过多个“头”进行处理，每个“头”都有自己的权重矩阵。然后，将所有头的输出拼接，通过线性变换产生最终的输出。 对于每个头，使用不同的权重矩阵对每个元素生成query和key 对单个元素，在每个头下，算出该元素的query和其他元素的key的点积，运用softmax函数算出注意力分数 将所有头的输出拼接在一起，对拼接的输出进行线性变换，得到最终的输出。 在自然语言处理任务中，一个头可能专注于捕捉句法依赖，另一个头可能专注于捕捉语义依赖。这种机制使模型能够更好地理解和生成复杂的文本。 Encoder Encoder将输入转换为“上下文向量” Encoder由六层identical layers组成，每一层都由多头自注意力机制和前馈神经网络组成。 Decoder Decoder接收编码器产生的上下文向量，将其转换为目标输出 Decoder同样由六层identical layers组成，但每一层都在encoder的layer中针对encoder的输出添加了一个注意力机制。 这个额外的注意力机制使得decoder在生成每个输出词的时候，能够关注到输入的所有部分 Why Self-Attention计算效率 RNN和LSTM的计算无法并行化。相比之下，Transformer的自注意力机制可以同时处理所有时间步，因此其计算可以高度并行化，提高计算效率。 捕捉长距离依赖 Transformer通过自注意力机制可以直接捕获序列中任何两个位置之间的依赖关系，无论距离多远。 全局上下文理解 Transformer中的每个元素的表示都由整个序列中的所有元素共同决定，这使得Transformer能更好地理解全局上下文。 更易于优化 由于RNN和LSTM的递归性质，它们在优化上可能面临梯度消失和梯度爆炸等问题。 同时RNN和LSTM需要一个一个处理时间步，而Transformer可以并行处理所有时间步，这使得Transformer的计算更小，可以使用标准的反向传播算法进行优化。 GPT发展史GPT结构参考Improving Language Understanding by Generative Pre-Training GPT发展 模型名称 训练方法 特点 论文 GPT-1 语言建模（预训练+微调） 建立了GPT系列的核心架构；确定了使用上文预测下一单词的基本原则 Improving Language Understanding Generative Pre-Training GPT-2 语言建模，扩大模型参数到1.5B 多任务语言模型 Language Models are Unsupervised Multitask Learners GPT-3 语言建模，扩大模型参数到175B 提示补全，上下文学习，Few-shot Learning Language Models are Few-Shot Learners Instruct-GPT 有监督微调，RLHF 指令回答，Zero-shot Learning Training language models to follow instructions with human feedback Codex 使用代码+文本训练 代码生成、chain-of-thought (CoT) Evaluating Large Language Models Trained on Code ChatGPT 使用对话数据进行强化学习指令微调 对话历史建模 GPT-4 使用多模态数据 更强的性能、接受图像输入 GPT-4 Technical Report 提示(prompt)补全：根据输入的提示词，补全提示词的句子。 上下文学习（in-context learning）：在输入中包含目标任务的描述或示例和正常输入测试用例，即使训练中不包含该任务数据，模型仍然可以产生符合期待的输出。 reinforcement learning from human feedback（RLHF）：使用人工反馈对模型进行微调，使用这种微调可以将模型分化到不同技能上，如Chatgpt更擅长对话。这种微调并不会为模型注入新能力，而是解锁或引出原本存在在模型中的能力（在预训练中获得）。同时，人类微调实际上会导致模型在benchmark上的性能下降，但能够使模型向人类对齐，生成更符合人类需求的输出（零样本问答、道德约束、知识边界认知）。 指令回答(Responding)：在经过RLHF之前，模型输出大部分是训练集中常见的提示补全模式。现在模型能够输出对prompt的合理回答，而不是相关却无用的句子。 chain-of-thought (CoT)：常规的prompt策略基于上下文学习能力，在提问前为模型提供（问题-答案）文本对的样本。CoT在常规prompt的基础上，增加了从问题到答案的自然语言推导过程，从而提高模型的逐步推导能力。这种能力很可能使代码训练的副产物。目前只能从实验的角度证明相关性：GPT-3不能进行思维链推理，经过RLHF的GPT-3版本思维链推理同样很弱，而针对代码训练的模型具有很强的语言推理能力。 对话历史建模：ChatGPT使用对话数据集进行微调和RLHF，使得GPT具备了记忆过往对话的能力。 多模态模型：能够接受图像输出，理解图像内容。 GPT-2 参数数量提高到15亿 单向语言模型，采用了多任务方式，进一步提升模型的泛化能力 GPT-3 参数数量提高到1750 亿 强大的零样本和少样本学习能力 生成的文本质量非常高，实验表明人们很难区分其生成的文本是否由人类编写 GPT-4Scaling Law KM scaling law. (2020) Chinchilla scaling law. (2022) LLM优越性为什么GPT-3没有引起人们普遍的关注？GPT-3在2020年出现，但直到2022年LLM才进入大众视野。从实际性能上看，最初版本的GPT-3哪怕是其中最大的175B模型，其表现也未能超越经过微调的预训练语言模型（PLM）。而实验观测得到的比例定律让模型性能与规模挂钩，这意味着即使更改模型架构，也无法突破参数规模的限制，令模型难以取得重大的性能突破。 然而在2022年，Chain of Thought(CoT)和instruction tuning的出现打破了比例定律，使得LLM在规模有限的情况下出现了性能上的飞跃。 instruction tuning：使用指令数据集（包含指令形式的任务描述、输入输出对、几组示例）对模型进行fine-tune。而RLHF对应的微调被称为alignment tuning Emergent Ability凝聚态物理里面常用涌现一词（英文emergent）来描述随着粒子数目增多突然出现的奇异现象。 涌现能力定义：小模型没有，只有大模型才有的能力。 涌现能力体现之处很多，我们重点关注的是那些（1）NLP领域长期努力但未能实现的；（2）其他NLP模型难以解决的；（3）关系到自然语言本质的； （不同的综述对重要的涌现能力的定义存在微妙差异，但总体方向相似） In-context learning (ICL)上下文学习能力最早在GPT-3中引入。假设现在有一个新的任务，只需要给语言模型提供任务描述或几个示例，它可以产生预期的输出，而不需要额外的训练和梯度更新。 Instruction following模型经过指令微调后，可以只根据指令形式的任务描述执行新任务，而不需要给出具体示例。最新研究中，想要实现指令服从，至少需要62B的模型。 不过对较小的模型，指令微调仍然可以能提高模型的性能。 同时，指令微调可以显著改善模型的使用体验。 根据测试人员对模型在真实问题（从LLM API中收集）下回答的评价，强化学习（PPO）和监督微调（SFT）后的模型回答质量显著高于微调前。 Step-by-step reasoning小型的语言模型通常很难解决需要多个推理步骤的复杂任务，如数学类相关的问题。通过使用chain-of-thought (CoT)策略，把带有中间步骤的样例加入到prompt中，LLM能够学习逐步推理直到产生最终答案的能力。 人工CoT： 输入 问题：小明有5个乒乓球，又买了2罐子乒乓球，每个罐子有3个乒乓球。请问小明现在有多少个乒乓球？ 回答：小明从5个乒乓球开始，2个罐子，每罐3个，总共6个乒乓球。5+6=11。答案是11。 问题：锯一根10米长的木棒，每锯一段要2分钟。如果把这根木棒锯成相等的5段，请问需要多长时间？ 回答： 输出 把木棒锯成5段，需要4次。锯一次2分钟，4次需要4*2=8 分钟。答案是8分钟。 Zero-shot CoT： 输入 问题：小明有5个乒乓球，又买了2罐子乒乓球，每个罐子有3个乒乓球。请问小明现在有多少个乒乓球？ 回答：Let’s think step by step. 输出 Let’s think step by step. 把木棒锯成5段，需要4次。锯一次2分钟，4次需要4*2=8 分钟。答案是8分钟。 Auto-CoT 聚类选取有代表性的问题 对于每一个采样的问题拼接上“Let’s think step by step”（类似于 Zero-Shot-CoT ）输入到语言模型，让语言模型生成中间推理步骤和答案，然后把这些所有采样的问题以及语言模型生成的中间推理步骤和答案全部拼接在一起，构成少样本学习的样例，最后再拼接上需要求解的问题一起输入到语言模型 这种方法显著提高了LLM解决问题的能力。根据实验分析，当模型规模大于62B时，CoT才比直接回答问题存在优势。如果模型规模较小，CoT反而会导致性能降低。（根据模型和任务的不同，这个临界值有差异） CoT能力来源分析 CoT出现的必要条件是模型达到一定规模，但规模并不是CoT的充分条件。 在175B的初始GPT-3、OPT、BLOOM等模型中都没有表现出CoT能力。（表现出CoT指采用CoT prompting后，模型性能比普通prompting和微调T5-11B更高） CoT与训练集中的代码数据相关。 使用代码数据作为训练集，大小为GPT-3数据集28%的CodeX，展现了较强的链式推理能力。 CoT与指令微调相关 GPT-3版本text-davinci-001链式推理能力差，而经过强化学习指令微调后的版本text-davinci-002，模型链式推理能力显著增强。 其他涌现能力（2022/8） Few-Shot Prompted Tasks. 在模型达到一定规模后，能通过few-shot prompting策略在benchmark上取得突破性的准确率增长 Augmented Prompting Strategies. 能够增强LLM的prompting技巧。如果某些技巧在模型达到一定程度才能起作用，在此之前都没有效果甚至有害，那也称其为涌现能力 注意，一个LLM并不一定包含所有涌现能力，包含涌现能力也不说明LLM一定优于PLM。 Prompt Engineeringsystem prompt引导模型生成与任务相关的响应 user prompt为模型提供了具体的问题或要求 通过不断优化提示来提高模型的性能。 这种技术的优点是可以简单的对模型进行微调和定制，而无需重新训练整个模型。","link":"/2023/06/07/%E3%80%90%E6%80%BB%E7%BB%93%E3%80%91LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E5%85%A5%E9%97%A8%E7%AF%87%EF%BC%89-%E7%AC%AC%E4%B8%80%E5%91%A8/"},{"title":"【论文导读】GLM：General Language Model Pretraining with Autoregressive Blank Infilling","text":"NLP的主流架构自回归模型（GPT）： 特点：自回归模型通过从左到右逐个生成文本的方式，具有较强的生成能力，能够生成长文本。在训练过程中，可以利用上下文信息进行预测，从而捕捉到一定的语义关联。 优点：生成效果较好，能够生成连贯的、上下文相关的文本。适用于无条件生成任务，如生成文章、对话等。 缺点：由于是单向模型，无法直接获取后文信息，可能导致在某些需要全局信息的任务中表现不佳。生成过程相对较慢。 自编码模型（BERT）： 特点：自编码模型使用双向Transformer作为编码器，在文本理解任务中具有较好的表现。能够利用上下文信息建模词语之间的关系，获取丰富的语义表示。 优点：具有较强的语言理解能力，适用于下游任务，如文本分类、分词、句法分析、信息抽取等。可以直接应用于许多NLU任务，而无需额外微调。 缺点：在文本生成任务中，由于缺乏显式的生成机制，无法直接生成文本。 编码解码模型（T5）： 特点：编码解码模型使用双向注意力的编码器和单向注意力的解码器，并通过交叉注意力连接两者。在有条件生成任务（seq-seq）中表现良好，如文本摘要、回答生成等。 优点：既具备编码器的语言理解能力，又能通过解码器生成文本，可适用于多种任务。能够综合编码解码的优势，实现一定程度上的任务统一。 缺点：相比于专门针对某一任务的模型，编码解码模型在某些特定任务上的性能可能不如专门优化的模型。 NLP的主流任务自然语言理解 文本分类：将给定的文本分为不同的预定义类别。 分词：将连续的文本序列划分为有意义的单元（词、子词等）。 信息抽取：从文本中抽取结构化信息，如实体关系、事件等。 无条件生成 文本生成：在没有给定特定条件的情况下，生成连贯、有意义的文本，如文章、故事、对话等。 语言建模：通过预测下一个词语或字符来生成自然语言序列，用于语言模型训练和生成文本。 条件生成 机器翻译：将一种自然语言的文本翻译成另一种自然语言的文本。 文本摘要：将长文本压缩成简短的摘要，捕捉主要信息。 生成式推理：在给定条件的情况下，生成符合逻辑的推理结果。 自然语言理解 条件生成 无条件生成 自回归模型 —— —— ✓ 自编码模型 ✓ ✕ ✕ 编码解码模型 —— ✓ —— **注释**：✓表示擅长；✕表示无法应用；——表示可以应用 总而言之，没有一个框架能同时做到对三个主流任务都表现最佳。论文作者由此提出了GLM（Generalized Language Model）模型，实验结果表明，在相同的模型大小和数据量下，它在自然语言理解、有条件生成和无条件生成等任务上都优于BERT、GPT和T5等模型。 GLM的主要特点 自回归空白填充：GLM通过在输入文本中随机选择一段连续的token并将其置为空白（Blank），然后使用自回归的方式重新构建这段空白内容。这种方法使得GLM能够同时进行语言理解和生成任务，通过预测空白部分来捕捉上下文信息。 二维位置编码：GLM引入了2D位置编码技术，用于表示跨度（span）内部和跨度之间的信息。这种编码方式使得GLM能够更好地建模不同位置之间的关系力。 随机顺序预测跨度：GLM在训练过程中允许以随机的顺序预测跨度，即可以先预测跨度的起始位置，再预测结束位置。这种随机顺序的预测能够增强模型对长跨度和复杂跨度的建模能力。 自回归空白填充 自回归空白填充通过对输入文本中的一些连续片段进行随机掩码处理，然后以自回归的方式预测被掩码的部分 注释：[M] := [掩码]，[S] := [开始]，[E] := [结束] 数据处理： 从输入文本中随机采样多个文本片段，每个片段是一段连续的标记序列。 将选中的文本片段替换为特殊的 [MASK] 标记，形成一个损坏的文本。 将损坏的文本和被替换的文本片段组成的部分称为 Part A，其余的文本片段构成的部分称为 Part B。 自回归建模： GLM以自回归的方式从损坏的文本中预测被掩码的标记。在预测一个片段中被掩码的标记时，模型可以访问损坏的文本和之前预测的片段。 为了捕捉不同片段之间的相互依赖关系，片段的顺序会被随机打乱，类似于置换语言模型。 模型按照从左到右的顺序逐个生成片段中被掩码的标记。 输入与输出： 将 Part A 和 Part B 拼接在一起作为 GLM 模型的输入。 每个被掩码的文本片段在输入时前面加上 [START] 标记，在输出时后面加上 [END] 标记。 通过二维位置编码来表示片段之间和片段内部的位置关系。 自注意力机制： 在自注意力机制中，Part A 中的标记只能与自身的标记相互关注，而不能关注 Part B 中的标记。（蓝框） Part B 中的标记既可以关注 Part A 中的标记，也可以关注自身之前的标记。（黄色框和绿色框） 论文作者表示，GLM的自回归空白填充方法使用泊松分布，采样出长度为 λ = 3 的跨度，并不断重复采样，以确保至少15%的标记被掩码。论文作者通过实验发现，这个15%的比例对于下游自然语言理解任务的良好性能至关重要。 多任务训练 GLM进行了多任务预训练，旨在训练一个能同时处理NLU和文本生成的单一模型。 文档级别任务（Document-level）：随机采样一个跨度（span），其长度从原始文本长度的50%到100%的均匀分布中抽样。这个目标旨在让模型能够生成长文本内容。 句子级别任务（Sentence-level）：限制掩码跨度必须是完整的句子。通过随机采样多个跨度（句子）来覆盖原始标记的15%。这个目标旨在进行序列到序列（seq2seq）任务，其中预测通常是完整的句子或段落。 这两个新的目标任务与自回归空白填充具有相同的定义方式，即通过最大化预测概率来优化模型参数。区别在于跨度的数量和长度。 模型结构 GLM使用了单个Transformer作为模型的主体架构，并对其进行了一些修改 重新排列了层归一化（Layer Normalization）和残差连接（residual connection）的顺序，这是为了避免数值错误，特别是在大规模语言模型中（如Megatron-LM）。 使用了单个线性层（linear layer）进行输出标记的预测。这个线性层用于生成模型的输出，即对下一个标记进行预测。 将ReLU激活函数替换为GELU激活函数。GELU（Gaussian Error Linear Unit）通过加权输入的值来非线性地激活，而不是像ReLU那样通过输入的符号来门控。选择GELU是因为神经元输入通常遵循正态分布，特别是在使用批归一化时。它在计算机视觉、自然语言处理和语音任务中都取得了改进效果。 微调在GLM中，为了进行下游的NLU任务微调，传统的做法是将预训练模型生成的序列或标记表示作为线性分类器的输入来预测正确的标签。然而，这种做法会导致预训练和微调之间存在不一致性。 为了解决这个问题，GLM借鉴了PET（Pattern-Exploiting Training）的思想，将NLU分类任务重新构造为空白填充的生成任务。 具体而言，对于给定的带有标签的示例（x，y），通过模式将输入文本x转换为一个完形填空问题c(x)，其中模式包含一个单独的掩码标记。模式以自然语言的形式编写，以表示任务的语义。 例如，情感分类任务可以表示为“{SENTENCE}。It’s really [MASK]”。候选标签y ∈ Y也被映射为完形填空问题的答案，称为verbalizer v(y)。在情感分类中，标签“positive”和“negative”被映射为单词“good”和“bad”。给定输入x的条件下，预测标签y的概率可以表示为： 其中Y是标签集合。句子是积极还是消极的概率与在空白中预测“good”或“bad”的概率成正比。然后，使用交叉熵损失对GLM进行微调。 分析与比较与BERT相比 BERT在处理连续的多个掩码标记时存在困难，因为BERT假设掩码标记之间是独立的，无法捕捉它们之间的依赖关系。 BERT不能正确地填充多个掩码标记的空白。对于预测长度为l的答案，BERT需要进行l次连续预测。如果答案的长度l是未知的，那么可能需要枚举所有可能的长度。 与XLNet相比GLM和XLNet都采用了自回归的预训练目标，但存在两个区别： XLNet在文本损坏之前使用原始的位置编码，在推断过程中需要知道或枚举答案的长度，这与BERT存在相同的问题。 XLNet使用了双流的自注意力机制来避免Transformer内部的信息泄漏，但这增加了预训练的时间成本。 与T5相比T5提出了类似的空白填充任务目标，但区别在于 T5在编码器和解码器中使用独立的位置编码，依赖于多个哨兵标记来区分不同的掩码跨度。 T5只使用一个哨兵标记，导致了模型能力的浪费和预训练微调的不一致性。 T5总是按照固定的从左到右顺序预测掩码跨度。相比之下，GLM在NLU和seq2seq任务上需要更少的参数和数据。 与UniLM相比UniLM通过在自动编码框架下改变双向、单向和交叉注意力之间的注意力掩码来结合不同的预训练目标。然而UniLM的缺点在于 UniLM始终将掩码跨度替换为[mask]标记，限制了它对掩码跨度及其上下文之间依赖关系的建模能力。对比之下，GLM通过输入先前的标记并自回归地生成下一个标记来实现预训练和微调。 UniLM在微调下游生成任务时也依赖于掩码语言建模，效率较低。然而，GLM通过自回归方式统一了NLU和生成任务。 实验SuperGLUE 注释：可以看出GLMBase 得分比BERT Base 高 4.6%，GLMLarge 得分比BERT Large 高 5.0%。 在多任务预训练的情况下，GLMDoc和GLMSent的性能略低于GLMLarge，但仍然优于BERTLarge和UniLMLarge。在多任务模型中，GLMSent的性能比GLMDoc平均高出1.1%。将GLMDoc的参数增加到410M（BERTLarge的1.25倍）可以获得比GLMLarge更好的性能。具有515M参数（BERTLarge的1.5倍）的GLM性能更好。 Sequence-to-Sequence 在上面表格中展示了在更大语料库上训练的模型的结果。GLMRoBERTa可以达到与序列到序列BART模型相媲美的性能，并且优于T5和UniLMv2模型。 在下面的表格中展示了在BookCorpus和Wikipedia上训练的模型的结果。观察到，GLMLarge在这两个生成任务上的性能与其他预训练模型相当。GLMSent的性能比GLMLarge更好，而GLMDoc的性能略低于GLMLarge。这表明，文档级目标对于条件生成任务的帮助较小，因为条件生成任务更注重从上下文中提取有用信息。将GLMDoc的参数增加到410M可以获得两个任务上最佳性能。 文字填充 GLM 大大优于以前的方法，1.3 到 3.9 BLEU。在此数据集上取得了最优秀的结果。 消融实验 作者进行了一系列的消融实验以评估GLM模型的不同组成部分对性能的影响 GLM模型在NLU任务上表现优于BERT，并且明显优于使用Masked LM预训练的BERT模型。 GLM相对于类似的cloze-style微调的BERT模型，在处理可变长度空白的任务上表现更好，特别是在包含多个标记的verbalizer的任务中。 对于大模型而言，采用cloze-style微调可以显著提高GLM在NLU任务上的性能。 在消融实验中，去除span shuffling和使用不同的哨兵标记代替单个[MASK]标记都导致GLM模型性能下降，这说明这些设计对于GLM的性能至关重要。 GLM与T5在预训练目标和设计上存在差异，但GLM在实验结果中展现了其优势。 总结论文作者提出了GLM（General Language Model），一种新的通用语言模型，通过自回归空白填充的方式实现了同时处理自然语言理解、有条件生成和无条件生成任务的能力。 GLM通过自回归生成被掩码的文本部分，结合二维位置编码和随机顺序预测跨度的方法，提升了模型对上下文信息和长跨度依赖关系的建模能力。 论文作者认为在多任务预训练和微调阶段，GLM的性能优于BERT、GPT和T5等模型。其设计的实验表明，GLM在多个NLU任务、文本生成任务和文本填充任务上都取得了优秀的结果，并且其性能明显优于其他模型。","link":"/2023/06/13/%E3%80%90%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E3%80%91GLM%EF%BC%9AGeneral%20Language%20Model%20Pretraining/"},{"title":"【论文导读】GPT-1：Improving Language Understanding by Generative Pre-Training","text":"模型结构GPT模型包含2个阶段：pre-training和fine-tuning。pre-training阶段从大量的无标签文本中学习建立语言模型。fine-tuning阶段基于标注的数据集针对下游分类任务进行训练。这种设计方案与BERT模型类似，但其实初代GPT的发布时间更早。 无监督预训练GPT与BERT主要的差异之一是GPT使用Transformer模型的decoder结构，而BERT使用encoder结构。 给定一个tokne样本 $U= {u_1,…,u_n}$，GPT使用标准的language modeling目标最大化以下函数： $$L(U)=\\sum_i\\log P(u_i|u_{i-k},…,u_{i-1};\\Theta)$$ 从模型的目标函数可以看出，GPT使用了单向的上文信息传递，这种方法比BERT的双向语言模型损失了部分信息，但个人认为这种上文信息+decoder的设计使得GPT更加适用于文本生成任务。 有监督微调现有数据集 C，其中每个用例包含输入tokens $x^1,…,x^m$ ，其标签为 $y$。基于预训练模型，可以获取最后一层transformer的激活序列 $h^m_l$。通过增加一层额外的线性输出层，训练其中的参数 $W_y$，GPT可以实现对数据集分类任务的微调，而不影响预训练中的参数。 $$P(y|x^1,…,x^m)=softmax(h^m_lW_y)$$ 得到待训练的目标函数： $$L_2(C)=\\sum_{(x,y)}\\log P(y|x^1,…,x^m)$$ GPT在此基础上增加了一个额外目标——language modeling，多项研究证明该目标可以提高监督模型的质量并加快参数融合。因此，GPT使用带权重 $\\lambda$ 的目标函数进行优化。 $$L_3(C)=L_2(C)+\\lambda*L_1(C)$$ 不同任务的输入转换在处理文本分类以外的任务，如问题回答，需要输入有序的句子对或三元组。由于模型基于连续的文本序列进行训练，这些结构化的数据需要进行额外的处理或更改模型结构。GPT提出一种遍历式方法，在数据处理中引入迁移学习，把结构化输入转换为有序序列。 信息蕴含：将前提和假设连接，中间插入分隔符 ($)，作为输入。 文本相似：两个句子分开处理得到两个 $h^m_l$，将两个序列逐元素相加的结果传到输出层。 问题回答和常识推理：给定三元组（文档$z$，问题$q$，一组可能的回答${a_k}$）。将文档和问题连接后，分别与每个可能答案使用分隔符连接得到 ${z;q;$;a_k}$ 。将所有得到的序列独立输入模型，通过softmax层对输出进行归一化，生成可能的答案上的概率分布。","link":"/2023/06/06/%E3%80%90%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E3%80%91GPT-1%EF%BC%9AImproving-Language-Understanding-by-Generative-Pre-Training/"},{"title":"【论文导读】Large language models are human-level prompt engineers","text":"介绍为了确保模型能够按照人类需求执行任务，prompt是非常有效的方式，然而构造prompt需要大量人工，本文提出了一个基于LLM生成和挑选指令的自动化算法。文中称问题为自然语言程序合成（Program Synthesis），利用LLM解决此黑盒优化问题，启发式地搜索可行解。 论文提出的算法 Automatic Prompt Engineer (APE) 流程如下： 使用LLM基于以少量输入输出样例，生成指令成员 使用LLM计算每条指令的得分，引导搜索过程 使用迭代蒙特卡洛方法，让LLM生成语义相似的变体尝试提高最佳指令的性能 程序合成（Program Synthesis）定义：自动搜索样本空间，寻找满足特定条件的程序 Program Synthesis本身是代码自动生成领域的一个概念，其核心问题是识别用户输入的意图（输入内容-输出内容，代码使用的结构等），生成符合约束条件的代码。 领域内的问题如下： 如何在用户输入不完整或含糊的情况下，形成完整逻辑 如何避免代码中的脆弱性（或确保正确性），以及如何使用验证机制加速代码生成 如何从曾经符号执行方法过渡到深度学习方法 如何解决代码以外的程序生成问题（如自然语言） 应用 根据用户描述（特定逻辑模板、自然语言……）生成代码 智能对话（根据用户问题，识别意图，生成API调用链进行回答） 对LLM来说，自然语言程序合成是利用LLM结构在自然语言程序空间中进行搜索，而无需额外的结构化假设和组件库。 方法基本定义： 输入输出样例数据集 $D_{trian}={(Q,A)}$ 样本空间 $X$ 测试大语言模型 $M$ 指令 $\\rho$ 连接指令和原输入 $[\\rho;Q]$，作为模型 $M$ 的prompt，寻找让 $f(\\rho,Q,A)$ 得分最大化的指令 $\\rho$：$$\\rho^*=argmax_\\rho f(\\rho)=argmax_\\rho \\mathbb{E}_{(Q,A)}[f(\\rho,Q,A)]$$注意，$Q$ 可能为空，此时直接优化 $\\rho$ 产生输出 ${A}$ 的概率。 初始样本分布考虑到自然语言巨大的搜索空间和语言模型生成文本的多样性，找到正确的指令会非常困难。 为了约束样本空间，首先使用预训练的LLM产生一组候选指令，指导搜索过程。尽管初始生成的指令不可能完全符合 $(Q,A)$ ，需要，但可以让LLM生成对给定输入输出对得分最高的指令，即求 $P(\\rho|D_train,f(\\rho)\\space is \\space high)$。 为此文中提出了两种方法 Forward Mode Generation. 将上面描述的概率分布转变为自然语言形式，作为模型prompt。 Reverse Mode Generation. Forward模式生成指令本质上是对prompt内容的补充。然而指令实际上需要出现的输入的最前面，与forward插入在末尾的形式存在冲突。我们希望产生一条能够插入在任意位置的指令。 因此，作者提出了reverse模式，利用了LLM的填空能力（如T5、GLM、InsertGPT），推测文本中空缺的指令。 Customized Prompts. 作者认为根据分数计算函数的不同，合适的模板也不同，可以自行设计其他合适的prompt来生成指令。对应实验TruthfulQA的模板如下图所示。 分数计算函数文中提出了一个衡量模型生成数据与数据集对齐程度的分数计算函数，其计算策略如下： Execution accuracy. 使用执行准确率矩阵 $f_{exec}$ 来衡量指令的质量。通常情况下，执行准确率被定义为0-1损失函数（如果预测值与目标不相等则为1，否则为0），$f(\\rho,Q,A)=\\mathbb{1}[M([\\rho,Q])=A]$ 。在某些任务中，执行准确率被设为一个定值。 Log probability. 进一步提出一个更soft的评分函数（soft是机器学习中的一个概念。指避免对类别的绝对判断，而表达每种类型的可能性），假设它会通过在搜索低质量候选指令时提供更细粒度的信号来改进优化，尤其考虑了目标模型下给定指令和问题时，期望答案的对数概率 $P(A|[\\rho;Q])$。 Efficient score estimation. 为训练集中的每个样例对每条指令都计算分数的开销过大。为了降低计算成本，文中给高质量的指令更多计算资源，反之亦然。这通过多层计算策略实现，首先用一小部分数据集分析所有指令，对得分高于门槛的指令，使用数据集的非重复子集再次进行得分计算，取分数的均值。重复此过程直到剩下相对较少的指令成员，再使用完整数据集进行测试。 迭代样本分布显然，初始样本中的指令未必能产生一个好的指令集，要么缺乏多样性，要么因为不包含任何高分成员指令。因此，作者采用了迭代的指令集 $U$ 重取样方法——迭代蒙特卡洛搜索。 考虑在当前最佳候选指令的周围空间进行搜索，而不是从初始状态重新取样。这种方法产生高质量质量的可能性更高，称该方法为迭代APE。每个阶段，都会评估一组指令，并去除其中的低分候选，然后要求 LLM 生成与高分指令相似的新指令。这里使用的模型提示如下: 值得注意的是，尽管迭代搜索使指令的整体质量提高，但得分最高的指令在多个阶段中基本一致。所以可以认为迭代生成方式只能带来有限的提高，实验中默认采用不包含迭代搜索的APE方法。 APE效果分析作者从zero-shot表现、few-shot 上下文学习表现、zero-shot思维链表现和事实认知四个层面对APE的效果进行了分析。 指令归纳（Instruction Induction）指令归纳：由Honovich等人2022引入，向模型提供输入-输出对演示，要求生成描述输入-输出对的自然语言指令。 这一任务包括了从简单的阶段结构到相似和因果关系识别的自然语言理解领域多个方面。实验中在InstructGPT上对模型自己生成指令（Greedy）、人工指令、APE生成指令三个方法进行了对比。 Zero-shot Few-shot主要展示了APE生成指令与上下文学习能力结合前后的对比 BIGBench作者构建了一个包含 BigBench-Hard 中9个问题，累计包含21个任务的指令归纳Benchmark—— BIG-Bench Instruction Induction (BBII)。并在此基础上对APE+InstructGPT的效果进行了评估。 Zero-shot CoT通过APE，作者找到了一个新的CoT prompt——Let’s work this out in a step by step way to be sure we have the right answer.” 其效果比原prompt——“Let’s think step by step.”略有提升。 开源项目 https://github.com/keirp/automatic_prompt_engineer 这篇论文团队提供了开源代码，在里面可以翻到每条指令评分的实现方法。 整体流程都与论文中描述的一致，我好奇的地方在于评分函数的实现： 123456789while response is None: try: response = openai.Completion.create( **config, prompt=text) except Exception as e: print(e) print('Retrying...') time.sleep(5)log_probs = [response['choices'][i]['logprobs']['token_logprobs'][1:] 可以看到，指令评分实现的核心就是把输入输出套个模板丢进OpenAI提供的text-davinci-002接口，模型返回评分进行分析。而0-1损失函数实际上反而是在这个更soft的评价方法上建立的，即取一个阈值，达到则为好，反之为不好。","link":"/2023/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E3%80%91Large-language-models-are-human-level-prompt-engineers/"},{"title":"【论文导读】Lora：Low-rank adaptation of large language models","text":"Adapter和Prompting是LLM中常用的轻量级微调方案。 Low-Rank Adaptation提出不改变预训练模型权重，而在模型外添加旁路，用训练旁路中的矩阵模拟微调过程。这种方法的优点是不引入额外延迟，只需要将旁路参数与模型参数合并，用新参数推理结果即可。 背景——微调原理在2020年，一项由Aghajanyan等人进行的研究揭示了微调背后的原理。 fine-tune（微调）：基于规模较小的标注数据集，通过梯度下降等算法调整模型权重，将具有大量参数的预训练模型调整到指定任务上的过程 intrinsic dimensionality（本征维度）：目标函数达到精确解决优化问题所需的最小维度。 对预训练模型而言，衡量本征维度告诉我们在对每个任务进行微调时需要多少空余参数才能大概解决优化问题。 标准的预训练模型仅需少量参数就可以学习大量NLP任务，预训练本身是在为下游NLP任务最小化本征维度。因此，文章认为预训练实际上是一个学习压缩平均NLP任务的框架。 计算本征维度计算目标函数的精确本征维度非常困难，可以使用启发式方法计算上界。 假设 $\\theta^D=[\\theta_0,\\theta_1,…,\\theta_m]$ 是模型 $f(·,\\theta)$ 的D个参数。相比直接优化模型参数 $\\theta^D$ 的损失函数，子空间方法通过低维参数化d个维度对模型进行微调： $\\theta^D=\\theta^D_0+P(\\theta^d)$ 其中 $P:\\mathbb{R}^d\\rightarrow\\mathbb{R}^D$，将参数从低维d映射到高维D。Li等人提出了3种映射方法，分别是random linear dense projection（随即线性密集映射）、random linear sparse projection（随机线性稀疏映射）、通过 Fastfood transform 进行随机线性映射。 Direct Intrinsic Dimension（DID）文中主要采用 Fastfood transform，其定义为： $\\theta^D=\\theta^D_0+\\theta^dM，M=HG\\Pi HB$ $M$ 可以被分解为Hadamard 矩阵 $H$，具有独立标准正规项的随机对角矩阵 $G$，一个等概率±1项的随机对角矩阵 $B$，随机排列矩阵 $\\Pi$。微调过程中，只有 $\\theta^d$ 是变量，其他都是定值（ $M$ 使用Fast Walsh-Hadamard Transform计算）。如果我们将 $M$ 约束为二进制矩阵，那么本征维度的计算被转化为一个稀疏问题的连续松弛，称为Direct Intrinsic Dimension (DID) 方法。 定义达到训练完整权重矩阵效果的90%为令人满意的效果，目标是寻找最小的维度d，使得调整效果达到训练完整权重矩阵准确率的90%（如果完整模型训练后准确率85%，那么调整后目标准确率为0.9*85%=76.5%），称其为维度 $d_{90}$。 Structure Aware Intrinsic Dimension（SAID）更进一步可以对模型中每一层的参数进行本征维度计算，称为Structure-Aware Intrinsic Dimension (SAID) 方法： $\\theta^D_i=\\theta^D_{0,i}+\\lambda_i P(\\theta^{d-m})_i$ 对于m层，我们从子空间 $\\theta^d$ 中交换m个参数，对每一层联合学习 $\\lambda$。 本征维度效果 MRPC：包含约3700个训练样本的段落文本语义一致性预测任务 QQP：包含约363k个训练样本的问题文本语义一致性预测任务 结论 预训练模型效果越好，本征维度越小 训练集规模越大，本征维度越大 介绍现有方法缺点 完全更新模型参数：参数量过大，效率低下，存储所有微调后的模型相当困难 Adapter：引入推理延迟，需要在效率和模型质量之间权衡 prefix tuning（对prompt微调）：优化难度大，其性能在可训练参数增加时非单调变化。更重要的是，固定一部分序列长度必然会导致下游任务可用的序列长度减少，可能导致模型性能不如其他方法。 Inspiration：根据背景中描述的研究，学习后的过参数化模型实际上处于一个低“intrinsic dimension”上，作者假设模型adaptation（和fine-tune类似，将模型迁移到一个指定任务上）过程中的权重变化矩阵同样有一个较低的内在秩。 提出方法：通过优化密集层在适应过程中的变化的秩分解矩阵来间接地训练神经网络中的一些密集层，而保持预先训练的权重冻结。 方法神经网络包含许多密集层进行矩阵乘积。这些层中的权重矩阵通常具有全秩。当模型适应一个特定的任务时，研究表明预训练模型具有一个低“内在维度“，尽管被随机投影到较小的子空间，模型仍然可以有效学习。 因此，LoRA假设对模型权重的更新同样有一个低秩。现有预选连权重矩阵 $W_0\\in\\mathbb{R}^{d\\times k}$ ，将权重更新表示为 $W_0+\\bigtriangleup W=W_0+BA$，其中 $B\\in\\mathbb{R}^{d\\times r},A\\in\\mathbb{R}^{r\\times k}$，秩 $r\\ll min(d,k)$。在训练中，与训练权重矩阵保持不变，不接受梯度更新，而将$A,B$作为训练参数矩阵。 对于原来的推理变换 $h=W_0x$，LoRA定义声明为： $h=W_0x+\\bigtriangleup Wx=W_0+BAx$ 使用随即高斯函数初始化A，使用0初始化B，因此$\\bigtriangleup W$在训练开始时为0。 在Transformer结构中，自注意力模块有4个权重矩阵 $W_q,W_k,W_v,W_o$，MLP（多层感知器，一种前向神经网络）模块有2个权重矩阵。LoRA只微调 $W_q,W_k,W_v$ 中的一个，相当于为下游任务调整时只变更注意力权重，而不变化MLP模块。 实验效果对比实验在r=4的条件下运行 Fine-Tuning (FT)：对模型所有参数进行梯度下降调优 Bias-only or BitFit：只训练偏差向量 Prefix-embedding tuning (PreEmbed)：向输入token中插入特殊token，这些特殊token包含可训练的词嵌入。prefixing 前置到输入prompt，infixing 将token附加到prompt Prefix-layer tuning (PreLayer)：对PreEmbed的拓展，在每一层Transformer之后，使用可训练的连接层代替激活函数。可训练参数规模与Transformer层数相关。 Adapter tuning：Houlsby等人2019年提出在自注意力模块（或MLP模块）和后续连接层之间插入adapter层，adapter层包含2个全连接层，中间是非线性函数。此处称其为初始 $Adapter^H$。Lin等人2020年提出一个更有效的设计，旨在MLP模块和LayNorm操作后插入Adapter，称为 $Adapter^L$。另外两个不同的优化分别被称为 $Adapter^P$，$Adapter^D$。 RoBERT GPT-3 175B 其他因素影响基于GPT-3 175 进行实验 目标权重矩阵 秩r取值 总结从设计角度出发，LoRA就已经胜过之前的各种prompt tuning方法了，更不用说LoRA在实际使用中的种种便利，简单来说就是赢麻了。 不添加额外层和prompt，不引入延迟或prompt长度损失 与模型参数直接相关，相当于直接决定模型输出 可以任意调节参数大小 可以分级进行调节 更大程度保留了多任务能力 更强的可解释性","link":"/2023/06/09/%E3%80%90%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E3%80%91Lora%EF%BC%9ALow-rank-adaptation-of-large-language-models/"},{"title":"【论文导读】QLoRA：Efficient Finetuning of Quantized LLMs","text":"背景​ 我们知道，大语言模型可以通过大规模的无监督预训练来学习丰富的语言知识，并通过微调来适应不同的下游任务，从而在各种NLP任务上取得了令人瞩目的性能。但是，LLM也带来了一些挑战，其中一个便是它们的巨大规模和高昂的计算成本。例如，对650亿参数的LLaMA模型进行16位微调需要超过780GB的GPU内存，这远远超出了普通用户和研究者拥有的资源。虽然最近的量化方法可以减少LLM的内存占用量，但是这些技术仅适用于推理，并不适合在训练过程中使用。 ​ 在《QLoRA: Efficient Finetuning of Quantized LLMs》这篇论文中提出了一种针对LLM的低精度量化和高效微调技术，可以在保证完整的fp16的微调任务性能的同时，减少内存使用，从而能够在单个48GB显存的GPU上微调65B参数模型，大大降低了微调模型所需的内存。 基本原理 ​ 在上周的汇报中提到，LoRA的原理是为LLM的每一层都添加了少量的可训练参数（适配器），并冻结了所有原始参数。这样对于微调，只需要更新适配器权重，这样可以显著减少内存占用。 ​ 而QLoRA更进一步，引入了三个关键的技术： 4-bit NormalFloat (NF4)量化 ​ 量化（Quantization）是指用低精度数据类型去逼近神经网络中的高精度浮点数，以提高运算效率和减少内存占用。NF4量化是建立在分位数量化技术的基础之上的一种信息理论上最优的数据类型。NF4量化可以保证量化后的数据和量化前具有同等的数据分布。也就是通过NF4量化后，权重信息损失减少，那么最后模型的整体精度损失也减少。 双重量化（Double Quantization） ​ Double Quantization是将额外的量化常数进行二次量化以减小内存开销的过程。假设现在每64个参数块共享一个32bit的量化常数，这样的话相当于每一个参数的量化额外开销为32/64 = 0.5 bit。这个总体来说也是比较大的开销，所以为了进一步优化这个量化开销，我们对其进行二次量化(Double Quantization)，即把第一次32bit量化的输出作为第二次量化的输入，如果采用256的块大小对量化常数进行FP8量化，那么通过计算发现，我们可以把每个参数的量化额外开销降低到：$$\\frac{8}{64}+\\frac{32}{64*256}\\approx 0.127\\ \\mathrm{bit}$$ 分页优化器（Paged Optimizers） ​ 使用NVIDIA统一内存功能，在GPU偶尔运行内存不足的情况下，可在CPU和GPU之间自动进行页面到页面的传输，从而可实现无错误的 GPU 处理。其工作方式类似于 CPU RAM 和磁盘之间的内存分页。在 GPU 内存不足时，QLoRA会将优化器状态自动驱逐到 CPU RAM，并在优化器更新步骤中需要内存时，又将它们分页回 GPU 内存，从而保证训练正常进行下去。 ​ 这些步骤大大减少了微调所需的内存，同时性能几乎与标准微调相当。 实验结果","link":"/2023/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E3%80%91QLoRA%EF%BC%9AEfficient-Finetuning-of-Quantized-LLMs/"},{"title":"【论文导读】ReAct：Synergizing Reasoning and Acting in Language Models","text":"介绍ReAct：结合reasoning和acting。Reasoning允许模型进行模型归纳、跟踪和更新行动计划，甚至处理异常。Acting允许模型与外部资源（例如知识库或环境）交互并从中收集信息。 传统的思维链方法虽然让模型实现了逐步推理得到结果的过程，但整个推理都基于模型的内部表示，而无法从外部世界获取信息，这限制了他的推理能力和知识更新能力。同时思维链中还会产生事实幻觉和错误传播等问题。 论文发布时，现有的方法只是实现了模型与几个模块交互的简单任务执行，还没有关于推理和行动如何以协同方式结合来解决一般任务的研究，以及与单独推理或行动相比，这种组合是否能带来性能提升的调研。 论文的具体贡献如下： 提出了ReAct，在语言模型中实现协同推理和行动以解决一般任务 在不同benchmark上广泛实验，验证了ReAct在few-shot任务中的优势 进行了消融实验和分析，以了解在推理任务中采取行动和在交互任务中推理的重要性； 分析了 ReAct 在prompt下的局限性（即对reason和act的有限支持），并进行了初始微调实验，显示了 ReAct 通过额外训练数据进行改进的潜力。 原理考虑agent与环境交互以解决任务的一般设置。在时间阶段 $t$，agent从环境收到一个观察值 $o_t ∈ O$，基于策略 $\\pi(a_t|c_t)$ 执行动作 $a_t\\in A$，其中 $c_t = (o_1, a_1, · · · , o_{t−1}, a_{t−1}, o_t)$ 是agent的上下文。学习这样一个条件策略是非常困难的，如下图所示，常规方法无法解决需要对轨迹上下文进行复杂推理的QA任务（找出Apple remote的另一种控制设备）。 相似的，下图中单纯的动作执行没有充分理解上下文，误解了胡椒瓶所在的位置。 ReAct的想法很简单：将agent的动作空间扩大 $\\widehat{A}=A\\cup L$，其中 $L$ 是语言空间，语言空间的一个动作 $\\widehat{a_t}\\in L$ ，称之为思维（thought）或推理轨迹（reasoning trace），不影响外部环境，因此也不需要观察反馈。思维 $\\widehat{a_t}$ 旨在通过推理当前上下文 $c_t$ 来组成更多的有价值信息，并更新上下文 $c_{t+1}=(c_t,\\widehat{a_t})$ 来支撑下一步推理或动作。思维有多种作用，如分解任务目标、导入任务相关的常识信息、提取观察中的重点部分、跟踪进度并更新计划、解决报错并调整计划等等。 关键问题在于语言空间 $L$ 不受限制，需要强语言前提。文中主要关注通过few-shot prompt提供上下文示例来生成特定领域的动作和各种形式的语言思维以解决任务。对于高度依赖推理的任务，思维和动作交替进行，使得过程包含多个thought-action-observation步骤。而对于决策类型的任务（包含大量的动作），思维只需要稀疏地出现在最关键的位置，由模型决定思维和动作的出现位置。 知识密集型推理任务通过与维基百科 API 交互，ReAct 能够检索信息以支持推理，同时还使用推理来确定接下来要检索的内容，展示了推理和行动的协同作用。实验使用2个数据集——HotPotQA、FEVER。 动作空间论文设计了简单的维基百科API，它具有三种类型的操作来支持交互式信息检索： search[entity]：如果存在，则返回相应实体 wiki 页面的前 5 个句子，否则从维基百科搜索引擎返回前 5 个相似实体； lookup[string]：它将返回包含string的页面中的下一个句子，模拟浏览器上的 Ctrl+F 功能； finish[answer]：用answer结束当前任务。 值得注意的是，这个动作空间只能根据准确的段落名称检索段落的一小部分，这明显弱于最先进的词汇或神经检索器。这样设计的目的是模拟人类与维基百科的互动方式，并强制模型通过语言的显式推理进行检索。 方法对于 HotpotQA 和 Fever，我们从训练集中随机选择 6 个和 3 个案例，手动构建ReAct格式的思维轨迹，组成few-shot prompts，每条轨迹都由多个思维-行动-观察步骤组成。如上文描述的，自由形式的思维用于实现不同的任务，如分解问题、提取信息、执行常识/算术推理、指导搜索公式和综合最终答案。 由于手动标注大规模推理过程和动作非常困难，考虑使用3000个现有的思维轨迹微调较小的语言模型 (PaLM-8/62B) ，用于基于输入解析思维轨迹。 实验效果 Standard：只给出问题-回答作为示例 Chain-of-thought（CoT）：移除动作和观察 Self-Consistency Chain-of-thought（CoT-SC）：在CoT的基础上使用不同结构的推理路径，生成多个答案，取其中出现次数最多的。 上述方法中，ReAct所展示的问题解决过程更具事实性和基础性，而CoT在制定推理结构方面更准确，但很容易产生幻觉的事实或想法。文中结合ReAct和CoT-SC，并让模型根据以下启发法决定何时切换到另一种方法 ReAct → CoT-SC：当ReAct没有在规定步数内返回结果时，采用CoT-SC CoT-SC → ReAct：当n个CoT-SC给出的答案中主流答案占比低于二分之一时，采用ReAct 决策型任务在ALFWorld和WebShop这两个基于语言的交互式决策任务上测试了ReAct，两个任务都具有复杂的环境，代理需要在长期动作中以稀疏的奖励激励行动，强调推理来进行高效探索。 ALFWorld：文本类游戏，它包括6种类型的任务，在这些任务中，代理需要通过文本动作（例如去咖啡桌1、拿纸2、使用台灯1）导航和与模拟家庭互动来实现高级目标（例如在台灯下检查纸张）。 WebShop：在线购物网站环境，包含118万个真实世界的产品和12千条人工指令。与ALFWorld不同，网店包含多种结构化和非结构化文本，通过网络互动购买产品 实验效果 ALFWorld Inner Monologue（IM）：内心独白，所有动作前增加一段“内心独白”，与思维不同的是，IM受限于对环境的观察和实现目标需要满足的条件，而推理轨迹可以稀疏存在且支持更丰富的类型。 ReAct-IM：使用IM替换思维（内部推理）。由于缺乏高层目标分解，ReAct-IM在确定子目标何时完成或下一个子目标应该是什么时经常出错。此外，由于缺乏常识性推理，许多ReAct IM轨迹难以确定物品可能在ALFWorld环境中的位置。 WebShop imitation learning（IL）：模仿学习，训练机器模仿人类的连续动作，与环境交互 reinforcement learning（RL）：强化学习，通过奖励函数提高机器在特定指标上的表现 LangChain ReActLangChain中集成实现了ReAct框架，可以直接使用ReAct来建立agent执行任务，结合LLM和其他工具。官方文档 1234567from langchain.agents import load_toolsfrom langchain.agents import initialize_agentfrom langchain.agents import AgentTypefrom langchain.llms import OpenAIllm = OpenAI(model_name=&quot;text-davinci-003&quot; ,temperature=0)tools = load_tools([&quot;google-serper&quot;, &quot;llm-math&quot;], llm=llm)agent = initialize_agent(tools, llm, agent=&quot;zero-shot-react-description&quot;, verbose=True)","link":"/2023/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E3%80%91ReAct%EF%BC%9ASynergizing-Reasoning-and-Acting-in-Language-Models/"},{"title":"【论文导读】The Power of Scale for Parameter-Efficient Prompt Tuning","text":"介绍prompt design：设计特定的任务表述或数个样例，附加到原本模型输入。不需要对预训练模型进行微调，也不产生新模型。 缺点： 任务描述容易出错，且需要人工参与 prompt的有效性受到与模型输入匹配程度的约束 Soft PromptSoft Prompt：通过反向传播学习，可以被调整用于合并来自任意数量的标注样例的信号。 prefix tuning. 由Li等人在2021年提出，在生成式任务中效果优秀。这一方法保留了模型参数，并通过反向传播错误，调整每个encoder层和输入层前置入的prefix激活函数。从而对不同任务的微调模型只需要保存其对应的prefix参数。 prompt tuning. 保留完整预训练模型，仅允许每个下游任务有额外的k个可调整token置入于输入文本的头部。该方法prefix tuning的差别在于，他没有在中间层插入prefix或添加额外的输出层，仅仅对输入的prompt进行了调整。 prompt tuning设计生成式分类prompt tuning采用和t5类似的方法，将所有任务映射为文本生成。 相比对类型基于输入进行概率建模 $Pr(y|X)$，文本分类任务被建模为条件生成，Y是表示类别标签的一个序列，prompting 会在输入前插入一系列tokens，P。prompt tuning提出使用单独的可更新参数来决定P的内容。由此得到分类概率建模：$$Pr_{θ;\\theta_P}(Y |[P ; X])$$其中 $\\theta$ 是模型参数，在优化过程中固定，只改变 $\\theta_P$，来最大化概率函数。给定一组n个tokens，${x_1,x_2,…,x_n}$。首先T5会嵌入输入序列形成矩阵 $X_e$，而prompt tuning在头部插入的prompt也被嵌入，形成矩阵 $P_e$，最终输入模型的是矩阵 $[P_e;X_e]$，相当于通过更新矩阵 $P_e$ 来最大化目标概率。 实现 “Span Corruption”：基于T5的预训练模型进行实现，T5的预训练进行句子空缺内容的预测，使用哨兵标注空缺位置，下面举个例子。 输入：Thank you 〈X〉 me to your party 〈Y〉 week 输出：〈X〉 for inviting 〈Y〉 last 〈Z〉 其中输入中类似〈X〉的哨兵标注了空缺，输出表示对输入内容的填补，同样使用哨兵作为输出结尾 ”Span Corruption + Sentinel“：prompt tuning为了接近输入在预训练中的状态，微调时，向所有下游任务头部插入一个哨兵 “LM Adaptation”：延续T5进行少量额外的自监督训练，但以“LM”为目标（即Transformer中的解码器，基于上文预测下一个出现的token）。 作者希望通过LM Adaptation，把模型速转换成一个更类似GPT-3的模型。 由于这种方法与从头开始预训练的效果从未被比较过，文中尝试了不同量的Adaptation。 实验结果使用额外的100K步训练 LM-adapted 版本的T5模型，并设置prompt长度为100个tokens，这种长度需要的参数数量仍然比其他方法需要的更少。 每个prompt参数训练只基于单个SuperGLUE任务，不支持多任务设置。训练时，将每个任务的名称插入到输入的头部来标识样例归属。 方法效果比较 方法参数量比较","link":"/2023/06/09/%E3%80%90%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E3%80%91The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning/"},{"title":"【论文导读】The RefinedWeb Dataset for Falcon LLM","text":"介绍为了最大程度在规模扩张时提高模型性能，根据Chinchilla scaling law缩放定律（2022年由Deepmind提出），模型大小和数据集大小应该同时增加。而早期的KM scaling law，则认为应该首先关注模型大小，减少数据集的增大。 scaling law：以模型大小、数据规模和总计算量作为决定模型性能的关键要素，计算资源有限时三种要素的分配方式使得模型预期性能最大化。 根据Deepmind团队的描述，理想情况下训练一个175B模型需要至少35000亿token的文本。这种规模是目前最大预训练数据集的2倍，最大公开英文数据集的10倍。 为了满足这种需求，大部分数据都是通过网络爬取的方式获得，这种数据通常被认为质量低于人工审核的数据。该文章关注了数据质量对模型训练效果的影响，做出了以下贡献： 制作了一个高质量的包含5万亿token的英文数据集——REFINEDWEB 证明了仅使用网络数据就足够使模型性能（zero-shot能力）超过用人工审核数据集训练的模型 开源了RefinedWeb中的6000亿token和基于该数据集训练的1/7B模型 OpenLLM排名2023.6.19时的OpenLLMLeaderboard截图，可以看到falcon的排名很高，超越了LLaMA的同时，还有使用了更小的数据集 简而言之，在目前所有开源模型中，Falcon有最好的表现和相对较低GPU内存占用。 对于常规的任务部署来说，Falcon-40B-Instruct是最优选择。 数据集构建文中提出了MDR（MacroData Refinement），这是一个用于大规模过滤和消除CommonCrawl中web数据重复的管道。 MDR设计准则 Scale first. 规模优先。为了能够训练40-200B的模型，优先满足数据集规模达到万亿级别的需要。数据来源于CommonCrawl来避免领域知识单一 Strict deduplication. 严格去重。结合精准和模糊去重，实现一个严格的去重流水线 Neutral filtering. 中立过滤。为了避免在模型引入歧视信息，文中没有使用机器学习来过滤数据，而是使用规则和启发式方法，且仅用URL过滤成人内容 每一步过滤后剩余的数据量： 文档预处理 数据读取 CommonCrawl 提供两种形式WET文件（从网页中提取得到的纯文本）和WARC文件（原始HTML响应），由于WET文件通常存在保留了无关文本的问题，因此处理从WARC文件开始。 URL过滤 使用URL对成人网站（色情、暴力、赌博等）进行过滤，使用2个判断标准： 包含460万个域名的过滤列表 基于敏感词出现频率和权重计算的URL得分 Text extraction 文本提取 使用trafilatura库和正则表达式，实现保留HTML响应中主要的文字内容，去除无关的导航栏、广告等内容。 Language identification 语言识别 使用fastText语言分类器CCNet对语言分类，只保留英文占比高于0.65的文档。 文档级和行级过滤 Repetition removal 去除高重复内容 由于爬虫错误和低质量源，许多文件包含重复序列。文中采用2019由Rae等人提出的启发式方法去除了任何行、段或n-gram重复过多的文档。 具体实现： 对行和段落，分别计算它们在整个文档重复所占比例，以及重复部分中的每个字符重复出现的比例 对 $n\\in{2,3,4}$，计算出现频率最高的n-gram的占比 对 $n\\in{5,…,10}$，计算每一个发生重复的n-gram总占比 过滤所有比例大于门槛的文档。 Document-wise filtering 文档过滤 去除垃圾邮件、特殊字符序列等内容。 Line-wise corrections 行矫正 文档中的行有时会与不需要的内容交错，如（social media counters 3 likes, navigation buttons），文中使用行矫正过滤器，针对这些内容进行移除，如果移除内容超过文档的5%，则删除整个文档。 去重由于爬虫可能多次访问同一页面或页面抄袭，不同文档间的内容仍然存在重复。 Fuzzy deduplication 模糊去重 计算文档MinHash，衡量文档间的相似性，移除高度重复的文档组。 Exact deduplication 精准去重 在序列级别进行精准去重。通过使用后缀数组进行精准的逐token匹配（如特殊的免责声明或通知）。 URL deduplication URL去重 将数据集分为100个部分，在每一个部分中单独删除具有重复URL的文档。","link":"/2023/06/20/%E3%80%90%E8%AE%BA%E6%96%87%E5%AF%BC%E8%AF%BB%E3%80%91The-RefinedWeb-Dataset-for-Falcon-LLM/"}],"tags":[{"name":"auto-task","slug":"auto-task","link":"/tags/auto-task/"},{"name":"论文阅读","slug":"论文阅读","link":"/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"Prompt Engineering","slug":"Prompt-Engineering","link":"/tags/Prompt-Engineering/"}],"categories":[],"pages":[]}